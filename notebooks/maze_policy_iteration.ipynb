{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration\n",
    "\n",
    "\n",
    "As a part of our quest to better understand reinforcement learning, we experiment with basic dynamic programming algorithms. \n",
    "\n",
    "In a previous experiment, presented in [another notebook](./maze_value_iteration.ipynb) , we covered _Value iteration_, another key dynamic programming algorithm. We illustrated an example implementation of the algorithm on a simple _grid world_ or _maze_. \n",
    "\n",
    "In addition, we introduced the necessary terminology and background for dynamic programming, briefly discussing for example Markov Decision Processess, policies and value functions.\n",
    "\n",
    "This notebook continues the quest by implementing _Policy Itaration_ algorithm and illustrates the algorithm using the same examples as before. As the basics were already covered in the notebook, they are not repeated here, although we lightly elaborate the concepts specific for our implementation.\n",
    "\n",
    "As before, implementation of the common `Maze`building blocks are imported from another notebook, see [here](./maze_basis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import ipynb.fs\n",
    "\n",
    "from ipynb.fs.defs.maze_basis import (\n",
    "    Maze, Movement, \n",
    "    plot_maze, plot_policy_actions, plot_state_rewards, plot_state_values,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief recap of policy evaluation\n",
    "\n",
    "As [discussed](./maze_value_iteration.ipynb#Policy-evaluation-and-update-rules), the task of computing state-value function for states under policy $\\pi$ is called _policy evaluation_.\n",
    "\n",
    "The _state-value function_ of a state $s$ under policy $\\pi$, denoted $V^{\\pi }$, is the expected discounted return when following the policy from state $s$ onwards:\n",
    "\n",
    "\\begin{equation*}\n",
    "V^{\\pi }\\!\\left ( s \\right ) =\n",
    "\\mathbb{E}_{\\pi}\\!\\left [\n",
    "    U_{t}\\vert S_{t}=s\n",
    "\\right ]\n",
    "\\end{equation*}\n",
    "\n",
    "The _Bellman expectation equation_ for $V^{\\pi }$, gives the relation of the value of a state and its successor states as: \n",
    "\n",
    "\\begin{equation*}\n",
    "V^{\\pi }\\!\\left ( s \\right ) =\n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,\\pi \\left(s \\right ) \\right )\n",
    "\\left( r\\left (s,\\pi \\left(s \\right ),{s}'\\right )  + \\gamma \n",
    "V^{\\pi } \\left ( {s}' \\right )  \\right ) \n",
    "\\end{equation*}\n",
    "\n",
    "As noted, we have an equation for each state $V^{\\pi}\\!\\left ( s \\right )$ that refers to values of subsequent states $V^{\\pi}\\!\\left ( {s}' \\right )$. For all states $s \\in \\mathcal{S}$, gives a system of $\\left |\\mathcal{S}\\right |$ simultaneous linear equations with $\\left |\\mathcal{S}\\right |$ unknowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix implementation of policy evaluation\n",
    "\n",
    "To solve the system using a matrix equation, we first define the following matrixes:\n",
    "\n",
    "\n",
    "> $\\mathbf{V}^{\\pi}$ is a $\\left |\\mathcal{S}\\right | \\times 1$ vector of state values where $\\mathit{v_{j}}=V^{\\pi }\\left ( s_{j} \\right )$\n",
    "\n",
    "> $\\mathbf{R^{\\pi}}$ is a $\\left |\\mathcal{S}\\right | \\times 1$ vector of rewards $\\mathit{r_{j}}= r\\left( s,\\pi \\left ( s \\right ),s_{j}\\right)$, when $S_{t}=s,A_{t}=\\pi \\left ( s \\right ),S_{t+1}=s_{j}$.\n",
    "\n",
    "> $\\mathbf{P}^{\\pi }$ is a $\\left |\\mathcal{S}\\right | \\times \\left |\\mathcal{S}\\right |$ matrix of transition probabilities  where each element $\\mathit{p_{jk}}$ represents $\\mathbb{P}\\!\\left ( S_{t+1}=s_{k}\\vert S_{t}=s_{j},A_{t}=\\pi \\left ( s_{j} \\right ) \\right )$\n",
    "\n",
    "Using the above, we can represent the Bellman expectation equation in a matrix form as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{V}^{\\pi}=\\mathbf{P}^{\\pi} \\left (\\mathbf{R}^{\\pi}+\\gamma \\mathbf{V}^{\\pi} \\right )\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{V}^{\\pi}=\\mathbf{P}^{\\pi}\\mathbf{R}^{\\pi}+\\gamma \\mathbf{P}^{\\pi}\\mathbf{V}^{\\pi}\n",
    "\\end{equation*}\n",
    "\n",
    "Solving for $\\mathbf{V}^{\\pi}$ yields\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{V}^{\\pi}=\\left (\\mathbf{I}-\\gamma \\mathbf{P}^{\\pi} \\right )^{-1} \\mathbf{P}^{\\pi}\\mathbf{R}^{\\pi}\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the term $\\mathbf{P}^{\\pi}\\mathbf{R}^{\\pi}$ is a $\\left |\\mathcal{S}\\right |$ dimensional vector that represents the expected immediate return received from state $s$ when following policy $\\pi$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}^{\\pi}\\!\\left ( R_{t+1}\\vert S_{t}=s,A_{t}=\\pi \\left ( s \\right ) \\right ) =\n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,\\pi \\left ( s \\right ) \\right )\n",
    "r\\left ( s,\\pi \\left ( s \\right ),{s}' \\right ) \n",
    "\\end{equation*}\n",
    "\n",
    "where we are taking advantage of the fact that in our simple maze problem, the rewards depend only on the state entered, and we can use a vector to represent the returns.\n",
    "\n",
    "The selected approach sets us to invert a $\\left |\\mathcal{S}\\right | \\times \\left |\\mathcal{S}\\right |$ matrix $\\mathbf{I}-\\gamma \\mathbf{P}^{\\pi} $, an operation with computational complexity of $O(\\left |\\mathcal{S}\\right |^{3})$. An alternative iterative approach for policy evaluation we chose not to follow - using the Bellman expectation equation as an update rule, the _Bellman expectation backup_ - was brefly discussed [earlier](maze_value_iteration.ipynb#Policy-evaluation-and-update-rules)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration algorithm\n",
    "\n",
    "_Policy iteration_ in an incremental algorithm to find the optimal policy $\\pi^{\\ast}$ for a fully defined MDP. The algorithm alternates between two steps, _policy evaluation_ and _policy improvement_. \n",
    "\n",
    "Policy evaluation step is performed as discussed above - we find the state values corresponding to current policy, the $V^{\\pi }\\!\\left ( s \\right )$ for all $s$. \n",
    "\n",
    "Policy improvement step then finds a new better policy ${\\pi}'$ by determining the best action in each state with respect to the state-value function of the policy derived during the policy evaluation phase as:\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\pi}'\\!\\left ( s \\right ) = \\underset{a}{\\mathrm{argmax}}\\;Q^{\\pi}\\! \\left ( s, a \\right ) \n",
    "\\end{equation*}\n",
    "\n",
    "The recursive relationship between $Q^{\\pi }$ and $V^{\\pi }$ discussed earlier enables us to determine ${\\pi}'\\!\\left ( s \\right )$ by using one-step look-ahead - referring to state-value function of subsequent states only. \n",
    "\n",
    "This phase of improvement is also described as '_acting greedily_' with respect to current policy and the resulting policy is a '_greedy policy_' with respect to the value function. Remember, that in the case of value iteration, a similar procedure was referred to as 'policy extraction'.\n",
    "\n",
    "Policy iteration algorithm is guaranteed to converge starting from any arbitary policy: It can be shown, that each improvement step improves the value function, and also that if improvements stop, then the optimal value function $V^{\\ast}\\!\\left ( s \\right )$ satisfying the Bellman optimality equation, and the corresponding optimal policy $\\pi^{\\ast}\\!\\left ( s \\right )$ have been found. \n",
    "\n",
    "As each iteration considers a changed and improved policy (unless the algorithm has converged), previously evaluated policies of lesser value are not revisited. Still, in theory, convergence could be slow as there are $\\left |\\mathcal{A}\\right |^{\\left |\\mathcal{S}\\right |}$ different policies to evaluate. In practice, convergence is often rapid.\n",
    "\n",
    "Pseudocode for the algorithm stands as follows:\n",
    "\n",
    "<span style=\"font-family: monospace;\">\n",
    "\n",
    "> **Policy iteration:**     \n",
    ">\n",
    "> initialize $k =0$, $V_{0}\\left ( s \\right ) \\in \\mathbb{R}$ and $\\pi_{0}\\!\\left ( s \\right ) \\in \\mathcal{A}$  for all $S$\n",
    "> \n",
    "> loop:     \n",
    ">> ***Policy evaluation:*** \n",
    ">>   \n",
    ">> solve $\n",
    "\\mathbf{V}^{\\pi}=\\left (\\mathbf{I}-\\gamma \\mathbf{P}^{\\pi} \\right )^{-1} \\mathbf{P}^{\\pi}\\mathbf{R}^{\\pi}\n",
    "$ for $V_{k}\\!\\left ( s \\right )$, using $\\pi_{k}\\!\\left ( s \\right )$\n",
    ">>\n",
    ">>\n",
    ">> ***Policy improvement:*** \n",
    ">> \n",
    ">>    \n",
    ">> for each $s \\in \\mathbb{S}$:\n",
    ">>> improve policy, $\n",
    "\\pi_{k+1}\\!\\left ( s \\right ) \\leftarrow \\underset{a}{\\mathrm{argmax}}\\; \n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( R\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V_{k} \\left ( {s}' \\right )  \\right )\n",
    "$\n",
    ">>\n",
    ">> $k = k + 1$  \n",
    ">    \n",
    "> until convergence, $\\pi_{k+1}\\!\\left ( s \\right ) = \\pi_{k}\\!\\left ( s \\right )$ for all $s \\in \\mathcal{S}$\n",
    "> \n",
    "> return $\\pi^{\\ast} = \\pi_{k+1}$\n",
    "\n",
    "</span>\n",
    "\n",
    "For the implementation, we need to maintain the current state values $V_{k}$ and the current policy $\\pi_{k}$. Storing the previous policy for comparison as implied above is strictly not necessary if we take note whenever policy improvement changes policy for a state and terminate when policy is stable; i.e. there are no more changes to $\\pi_{k+1}\\!\\left ( s \\right )$.\n",
    "\n",
    "Computational complexity is $O(\\left |\\mathcal{A}\\right |\\left |\\mathcal{S}\\right |^{2})$ per iteration for $\\left |\\mathcal{A}\\right |$ actions and $\\left |\\mathcal{S}\\right |$ states. However, solving the linear system adds a matrix inversion of complexity $O(\\left |\\mathcal{S}\\right |^{3})$ for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting iterative policy evaluation would replace the evaluation step with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace;\">\n",
    "\n",
    "> initialize $\\theta > 0$, $j, k =0$, $V_{0}\\left ( s \\right ) \\in \\mathbb{R}$ and $\\pi_{0}\\!\\left ( s \\right ) \\in \\mathcal{A}$  for all $S$\n",
    ">    \n",
    "> ***Policy evaluation:*** \n",
    ">\n",
    ">\n",
    "> loop: \n",
    ">>    \n",
    ">> for each $s \\in \\mathbb{S}$:\n",
    ">>> update $\n",
    "V_{j+1 }\\!\\left ( s \\right ) \\leftarrow\n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,\\pi_{k} \\left(s \\right ) \\right )\n",
    "\\left( r\\left (s,\\pi_{k} \\left(s \\right ),{s}'\\right )  + \\gamma \n",
    "V_{j }\\! \\left ( {s}' \\right )  \\right ) \n",
    "$\n",
    ">>\n",
    ">> $j = j + 1$\n",
    ">\n",
    "> until convergence, $\\left \\| V_{j+1} - V_{j} \\right \\|_{\\infty}<\\theta$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration implementation\n",
    "\n",
    "In the following, we illustrate an implementation of Policy Iteration algorithm. \n",
    "\n",
    "For clarity, rather than importing the notebook, we repeat shared code from Value Iteration algorithm implemetation here, but comment on the reused parts here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, `get_best_action` implementation is reused. The function that is used for state-value function calculation and for policy extraction in Value iteration is used for policy improvement step in the case of policy iteration:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi_{k+1}\\!\\left ( s \\right ) \\leftarrow \\underset{a}{\\mathrm{argmax}}\\; \n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( R\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V_{k} \\left ( {s}' \\right )  \\right )\n",
    "\\end{equation*}\n",
    "\n",
    "Note again that the implementation is lazy; For several equally good actions, the first one is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(maze, movement, state, state_values):\n",
    "   \n",
    "    best_action = None\n",
    "    best_value = -math.inf\n",
    "\n",
    "    for action in movement.actions:\n",
    "\n",
    "        val = 0\n",
    "\n",
    "        for move_direction, p_move in movement.get_direction_probs(action):\n",
    "\n",
    "            s_prime = movement.move_from(state, move_direction)\n",
    "\n",
    "            reward = maze.living_cost + maze.rewards[s_prime]\n",
    "            s_prime_value = state_values[s_prime]\n",
    "\n",
    "            val += p_move * (reward + maze.gamma * s_prime_value)\n",
    "\n",
    "        if val > best_value:\n",
    "            best_value = val\n",
    "            best_action = action\n",
    "\n",
    "    return best_value, best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extract_policy` is again shared code. The function performs a sweep through states and finds the greedy action for each state based on state values determined during policy evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(maze, movement, state_values):\n",
    "    \n",
    "    policy = {}\n",
    "    \n",
    "    for state in maze.get_iterator(\"states\"):\n",
    "        \n",
    "        if maze.terminal[state]:\n",
    "            continue\n",
    "        \n",
    "        best_value, best_action = get_best_action(maze, movement, state, state_values)\n",
    "\n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calc_transfer_probabilities` utilizes the `Maze` structure to account for walls and `Movement` to account noisy movement, and generates the matrix of state transition probabilities $\\mathbf{P}^{\\pi }$ for current policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_transfer_probabilities(maze, movement, policy):\n",
    "    \n",
    "    def get_state_index(state):\n",
    "        for z, s in enumerate(maze.get_iterator('states')):\n",
    "            if s_prime == s:\n",
    "                return z\n",
    "    \n",
    "    k = maze.state_count\n",
    "    p = np.zeros((k,k))\n",
    "\n",
    "    for s_index, state in enumerate(maze.get_iterator('states')):\n",
    "\n",
    "        if maze.terminal[state]:\n",
    "            continue\n",
    "\n",
    "        policy_action = policy[state]\n",
    "\n",
    "        for move_direction, p_move in movement.get_direction_probs(policy_action):\n",
    "            \n",
    "            s_prime = movement.move_from(state, move_direction)\n",
    "            s_prime_index = get_state_index(s_prime)\n",
    "            \n",
    "            p[s_index, s_prime_index] += p_move\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`policy_evaluation` implements the evaluation step of the algorithm for current policy in matrix form. Inversion of the matrix is performed using the Moore-Penrose pseudo-inverse as implemented in `numpy.linalg.pinv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(maze, movement, policy):\n",
    "    \n",
    "    rewards = np.array( [ maze.get_as_list('rewards') ] ).T\n",
    "    rewards += maze.living_cost\n",
    "    \n",
    "    k = maze.state_count\n",
    "    \n",
    "    p = calc_transfer_probabilities(maze, movement, policy)\n",
    "    \n",
    "    a = (np.eye(k) - maze.gamma * p)\n",
    "    \n",
    "    # print(f\"matrix a shape: {a.shape}, rank: {np.linalg.matrix_rank(a)}, det: {np.linalg.det(a)}, cond: {np.linalg.cond(a)}\")\n",
    "\n",
    "    b = p @ rewards\n",
    "\n",
    "    # path one: pseudoinverse\n",
    "    a_inv = np.linalg.pinv(a)\n",
    "    new_values = a_inv @ b\n",
    "\n",
    "    # path two: linalg.solve\n",
    "    # new_values = np.linalg.solve(a, b)\n",
    "    \n",
    "    # path three: least squares method\n",
    "    #new_values, residuals, rank, sv = np.linalg.lstsq(a, b, rcond=None)\n",
    "    \n",
    "    new_values_dict = {}\n",
    "\n",
    "    for i, state in enumerate(maze.get_iterator('states')):\n",
    "        new_values_dict[state] = new_values[i,0]\n",
    "    \n",
    "    return new_values_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 'acting-greedily' step of `policy_improvement` gently hides the fact that we are actually reusing policy extraction implementation from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(maze, movement, state_values):\n",
    "    \n",
    "    # when doing value iteration, this function is called extract_policy\n",
    "    \n",
    "    return extract_policy(maze, movement, state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could retain `distance` as a concept, and change to `ord=0` for `np.linalg.norm`, yielding the desired number of changed policy elements between updates. This is not used currently, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(list1, list2):\n",
    "    \n",
    "#    return np.linalg.norm(np.subtract(list1, list2), ord=np.inf) # ord=np.inf for vectors is max(abs(x))\n",
    "    \n",
    "    return np.linalg.norm(np.subtract(list1, list2), ord=0) # ord=0 for vectors is sum(x != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point to main loop allows us to terminate early based on max number of iterations (`MAX_ITERS`). \n",
    "\n",
    "Again, we may choose to plot intermediate results after each iteration by giving a suitable plotter function for `plotter` keyword argument, such as `plot_maze` utility function defined in Maze basis implementation notebook. As the policy is updated and available after each iteration, we can readily visualize both intermediate state-values and intermediate policy if so desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(maze, movement, initial_policy, *, MAX_ITERS=10, plotter=None):\n",
    "    \n",
    "    policy = initial_policy.copy()\n",
    "    \n",
    "    for i in range(0, MAX_ITERS): \n",
    "       \n",
    "        values = policy_evaluation(maze, movement, policy)\n",
    "        updated_policy = policy_improvement(maze, movement, values)\n",
    "\n",
    "        if plotter:\n",
    "            plotter(maze, values, updated_policy)\n",
    "\n",
    "        changed_count = sum([ policy[r] != updated_policy[r] for r in maze.get_iterator(\"states\") if r in policy and r in updated_policy ])\n",
    "\n",
    "        print(f\"ROUND {i}, {changed_count}\")\n",
    "\n",
    "        if changed_count == 0:\n",
    "            return values, policy\n",
    "\n",
    "        policy = updated_policy\n",
    "    \n",
    "    return values, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: The Canonical maze\n",
    "\n",
    "Follow the flow used with Value Iteration algorithm, we first illustrate the algorithm using the canonical maze as an example and, as before, intialize the maze with single wall cell and two final states. We start with `gamma=1`, `living_cost = -0.04` and `noise=0.2`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_config = {\n",
    "    'size': (3, 4),\n",
    "    'walls': [(1,1)],\n",
    "    'terminal_states': [(0,3), (1,3)],\n",
    "    'rewards': {\n",
    "        (0,3): 1,\n",
    "        (1,3): -1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = Maze(maze_config, gamma=1, living_cost = -0.04)\n",
    "movement = Movement(maze, noise=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use initial state-values of zero and extract the policy corresponding to initial values as initial policy. \n",
    "\n",
    "Note that the initial values are not needed by the algorithm, but are only used to create the initial policy. Thus we hope to have a similar starting point for the algorithm as we had with policy iteration earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initial_values = { state:0 for state in maze.get_iterator(\"states\") }\n",
    "initial_policy = extract_policy(maze, movement, initial_values)\n",
    "\n",
    "plot_maze(maze, initial_values, initial_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ready to run the algorithm, we hope to converge in 20 iterations and use `plot_maze` visualize state-values and corresponding greedy policy actions for each iteration step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "values, policy = policy_iteration(maze, movement, initial_policy,\n",
    "                                  MAX_ITERS=20, plotter=plot_maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(values)\n",
    "display({ r:Movement.action_names[policy[r]] for r in maze.get_iterator(\"states\") if r in policy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: a slightly more complicated maze\n",
    "\n",
    "Repeating our second example for policy iteration, we run through corresponding steps for a maze of medium complexity. We again use learning parameters `gamma=1`, `living_cost=-0.04` and `noise=0.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_config2 = {\n",
    "    'size': (5, 7),\n",
    "    'walls': [(1,1), (1,2), (1,3), (2,1), (3,1),\n",
    "              (3,3), (3,4), (3,5), (2,5), (3,5) ],\n",
    "    'terminal_states': [(2,3), (1,5)],\n",
    "    'rewards': {\n",
    "        (2,3): 1,\n",
    "        (1,5): -1\n",
    "    }\n",
    "}\n",
    "maze2 = Maze(maze_config2, gamma=1, living_cost = -0.04)\n",
    "movement2 = Movement(maze2, noise=0.2)\n",
    "\n",
    "plot_state_rewards(maze2, ax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize by finding an initial policy based on state value values of zero.\n",
    "\n",
    "This time, we omit the `plotter` argument and thus skip plotting intermediate value functions or policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_values2 = { state:0 for state in maze2.get_iterator(\"states\") }\n",
    "initial_policy2 = extract_policy(maze2, movement2, initial_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "values2, policy2 = policy_iteration(maze2, movement2, initial_policy2, MAX_ITERS=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_values(maze2, values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_actions(maze2, policy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final example: a complex maze\n",
    "\n",
    "As our third and final example, we tackle the same ludicrously complex maze as before, but this time using policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maze_config3 = {\n",
    "    'size': (8, 7),\n",
    "    'walls': [ (1,1), (1,2), (1,4), \n",
    "              (2,1), (2,4), \n",
    "              (4,3),(4,5), (4,6),\n",
    "              (5,2),\n",
    "              (6,3), (6,4), (6,5),\n",
    "              (7,1) ],\n",
    "    'terminal_states': [ (1,5), (2,2), (2,5), (4,1), (4,2), (5,1), (5,3) ],\n",
    "    'rewards': {\n",
    "        (1,5): -1,\n",
    "        (2,2): -1,\n",
    "        (2,5): -1,\n",
    "        (4,1): -1,\n",
    "        (4,2): -1,\n",
    "        (5,3):  1,\n",
    "        (5,1): -1,\n",
    "    }\n",
    "}\n",
    "maze3 = Maze(maze_config3, gamma=0.9, living_cost = -0.01)\n",
    "movement3 = Movement(maze3, noise=0.2)\n",
    "\n",
    "initial_values3 = { state:0 for state in maze3.get_iterator(\"states\") }\n",
    "initial_policy3 = extract_policy(maze3, movement3, initial_values3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_rewards(maze3, ax=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values3, policy3 = policy_iteration(maze3, movement3, initial_policy3, MAX_ITERS=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_values(maze3, values3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_actions(maze3, policy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing the execution\n",
    "\n",
    "To get in idea of relative computational cost, we time the execution of the algorithm for the final example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture c\n",
    "%%timeit\n",
    "\n",
    "values3, policy3 = policy_iteration(maze3, movement3, initial_policy3, MAX_ITERS=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = r'.*$'\n",
    "timeitresults = re.findall(regex, c.stdout)\n",
    "print(timeitresults[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [result for Value iteration](./maze_value_iteration.ipynb#Timing-it) was \n",
    "\n",
    "68.4 ms +- 1.85 ms per loop (mean +- std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "for ` C_LIMIT = 0.0001`, resulting in 39 iterations of the algorithm. This should be compared with 5 iterations needed for policy iteration above.\n",
    "\n",
    "Exact comparison of the two algorithms is difficult, however, as value iteration terminates based on value of $\\theta$ and the policy probably has converged before the state-values reach the target level of maximum change between iterations - we could use a significant share of computation time to fine-tune low decimals of value-function estimates. \n",
    "\n",
    "Nevertheless, the computational cost of policy iteration compares favorably despite the burden of matrix inversion during policy evaluation step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
