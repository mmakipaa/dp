{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value iteration\n",
    "\n",
    "As a part of our quest to better understand reinforcement learning, we experiment with basic dynamic programming algorithms.\n",
    "\n",
    "A [key textbook by Sutton and Barto](http://incompleteideas.net/book/RLbook2020.pdf) defines dynamic programming as referring to \"_a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP)._\"\n",
    "\n",
    "_Value Iteration_ is one such algorithms and this notebook implements the algorithm on a simple _grid world_ or _maze_ as an _environment_. \n",
    "\n",
    "So to start, we need to model the maze environment as a Markov decision process. We first introduce the maze as a Markov Decision Process, as is necessary for applying a dynamic programing algorithm. \n",
    "\n",
    "We also briefly cover the underlying theory and introduce the necessary concepts, such as \"optimal policies\" referred to in the definition quoted above.\n",
    "\n",
    "Towards the end of the notebook, we outline an implementation of the algorithm in Python and provide execution examples by beginning with the 'canonical maze', a common example used for instance in several university courses. We also illustrate a couple of slightly more complex mazes as additional examples.\n",
    "\n",
    "Implementation of the maze is reused from [another notebook](./maze_basis.ipynb).\n",
    "\n",
    "Another fundamental dynamic programming algorithm, _Policy Iteration_, is covered in the [following experiment](./maze_policy_iteration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import SVG\n",
    "\n",
    "from ipynb.fs.defs.maze_basis import (\n",
    "    Maze, Movement,\n",
    "    plot_maze, plot_policy_actions, plot_state_rewards, plot_state_values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze as a Markov Decision Process\n",
    "\n",
    "A maze or a grid-world is an environment consisting of grid-like states. We use the [maze implementation](./maze_basis.ipynb) to plot an illustrative image, showing the canonical maze below. \n",
    "\n",
    "The problem represented by the maze has different interpretations in academia: It could be taken as a model for parking a car in a slippery garage, or modeling movement of a robot in a room, or even a robot chasing diamonds while avoiding a scary pit of fire.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_config = {\n",
    "    'size': (3, 4),\n",
    "    'walls': [(1,1)],\n",
    "    'terminal_states': [(0,3), (1,3)],\n",
    "    'rewards': {\n",
    "        (0,3): 1,\n",
    "        (1,3): -1\n",
    "    }\n",
    "}\n",
    "maze = Maze(maze_config)\n",
    "plot_state_rewards(maze, ax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "So, we represent the maze as a _Markov Decision Process_ (MDP).\n",
    "MDP is a tuple $\\left( \\mathcal{S,A,P,R}, \\gamma  \\right)$, where\n",
    "\n",
    "> $\\mathcal{S}$ is the set of _states_<br>\n",
    "$\\mathcal{A}$ is the set of _actions_<br>\n",
    "$\\mathcal{P}$ defines the _transition probabilities_ between states<br>\n",
    "$\\mathcal{R}$ is the _reward function_ and<br>\n",
    "$\\gamma$ is a discount factor\n",
    "\n",
    "We cover each of these briefly in the following.\n",
    "\n",
    "In addition to the environment, we typically envision an _agent_, a decision maker navigating the maze environment one step at a time, trying the reach a goal, like finding a terminal state and, especially in the case of reinforcement learning, trying to learn the desired behavior from experience. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "The states in the case of a maze environment are the grid positions within the maze which the agent can reach - the light gray squares in the above picture, as well as the terminal states marked with green and red. \n",
    "\n",
    "Squares blocked by walls (dark grey ones) are not states of the MDP representing the maze as they cannot be entered. \n",
    "\n",
    "As can be seen from the above code snippet, in the implementing code the states are indexed by `(x,y)`-coordinate tuples, starting from `(0,0)` in the top left corner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "The set of available actions are NORTH, EAST, SOUTH and WEST. \n",
    "\n",
    "For instance, taking action NORTH in state `(x,y)` would mean that the agent attempts to move to state with index `(x-1,y)`. Similarly action EAST would mean attempted movement to state `(x,y+1)`\n",
    "\n",
    "The available actions are the same in all states, but attempting to move against a wall or outside of the maze will result in staying in the current state. \n",
    "\n",
    "Movement within the maze is noisy, however. Attempt to move to certain direction (e.g. _north_) may result in ending up left of the intended direction (e.g. _west_ if intended direction was _north_), or right of the direction (e.g. _east_).\n",
    "\n",
    "The fact that the environment is stochastic is a key point for dynamic programming and reinforcement learning. If there was no uncertainty, the problem of determining optimal actions would be trivial. This uncertainty is modeled as _transition probablities_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition probabilities\n",
    "\n",
    "For a Markov Decision Process, transition probabilities $P\\left({s}'\\vert s,a \\right) = \\mathbb{P}\\!\\left ( S_{t+1}={s}'\\vert S_{_{t}}={s}_{t},A_{_{t}}={a}_{t} \\right )$ are assumed to follow Markov property: The probabilities depend only on current state and action:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}\\!\\left({S}_{t+1}={s}'\\vert {s}_{t:0},{a}_{t:0} \\right) = \\mathbb{P}\\!\\left ( S_{t+1}={s}'\\vert S_{_{t}}={s}_{t},A_{_{t}}={a}_{t} \\right )\n",
    "\\end{equation*}\n",
    "\n",
    "As defined, we assume a perfect model of the environment. Thus, we need to be able to determine the transition probablities for different actions and states of the maze. \n",
    "\n",
    "For the maze at hand, transition probabilities can be deducted considering noisy movement within the maze, characterized by parameter $noise$: \n",
    "\n",
    "The agent will successfully move to intended direction with probablity\n",
    "\n",
    "$ P_{straight} = 1 - noise$\n",
    "\n",
    "And agent ends up left or right of the intended direction with equal probabilities\n",
    "\n",
    "$ P_{left} = P_{right} =  noise \\;/\\; 2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the two-state maze shown in the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(filename='img/simple_square_maze.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming $noise=0.2$ and attempting to take action NORTH in state S1 would result in following transition probabilities and state-transition diagram (for clarity, shown only for action NORTH):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}\\left ( S_{t+1}=S_{1}\\vert S_{_{1}}=S_{1},A=NORTH \\right )=0.8+0.1=0.9\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}\\left ( S_{t+1}=S_{2}\\vert S_{_{1}}=S_{1},A=NORTH \\right )=0.1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(filename='img/noisy movement_example.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward function\n",
    "\n",
    "Reward function $\\mathrm{R}:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S} \\rightarrow \\mathbb{R}$ defines the numerical value of reward received, $R_{t+1} = r\\left( s,a,{s}'\\right)$, when $S_{t}=s,A_{t}=a,S_{t+1}={s}'$. That is to say, reward received when in state $s$, the agent selects action $a$ and is transitioned to next state ${s}'$. Note the convention of associating the received reward with time step $t+1$.\n",
    "\n",
    "Although other approaches are common, we consider rewards deterministic (when $s,a$ and ${s}'$ are known, that is) and prefer the three-argument function notation, stating explicitly that the reward received when entering the subsequent state ${s}'$ depends on that state, but also on previous state $s$ and action $a$ chosen in the previous state. \n",
    "\n",
    "The expected reward received for state-action pair $s,a$ depends on the transition probabilities of the MDP: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}\\!\\left ( R_{t+1}\\vert S_{t}=s,A_{t}=a\\right ) =\n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "r\\left ( s,a,{s}' \\right ) \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "However, for the simple maze illustrated earlier, the reward just depends on the state entered. In the examples that follow, we typically use reward of -1 and 1 for entering bad and good terminal states, respectively and zero for entering all other states.\n",
    "\n",
    "In addition, there can be a constant _living cost_ that is added to reward in each time step. Living cost could represent the price the agent pays for exploring the maze (if negative), or reward the agent gets for surviving in the environment (if positive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discount factor\n",
    "\n",
    "Finally, the discount factor $\\gamma$, $0 \\leq \\gamma\\leq 1$, defines how future rewards are discounted when considering discounted returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies and value functions\n",
    "\n",
    "In addition to concepts introduced for a Markov Decision Process above, we still need to briefly discuss policies and value functions to set the stage for the application of dynamic programming methods and value iteration algorithm in particular.\n",
    "\n",
    "We could consider policies and value functions to be related to an agent: Value function defining how the agent values different states of the environment, policy defining how the agent acts in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies\n",
    "\n",
    "Policy $\\mathrm{\\pi}:\\mathcal{S}\\rightarrow \\mathcal{A}$ defines a mapping from each state to an action. In the following, we consider only _deterministic policies_, so that function $\\pi \\left( s\\right)$ returns the action $a$ for state $s$ according to policy $\\pi$. As the policy formulated this way does not change over time, it is considered a _stationary policy_.\n",
    "\n",
    "In the case of _stochastic policies_, we would use notation $\\pi \\left( a\\vert s\\right)$ to denote the probablity of selecting action $a$ in state $s$ when following policy $\\pi$.\n",
    "\n",
    "The following image illustrates a policy. An action, although randomly selected in this case from (NORTH, EAST, SOUTH, WEST), is associated with each state of the MDP, resulting in a deterministic policy. \n",
    "\n",
    "There would be in total $\\left |\\mathcal{A}\\right |^{\\left |\\mathcal{S}\\right |}$ policies, or 262 144 different different policies in the case of the canonical maze, considering that there no action can be selected in terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {state: np.random.choice(Movement.actions) for state in maze.get_iterator(\"states\") if not maze.terminal[state]}\n",
    "plot_policy_actions(maze, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function\n",
    "\n",
    "Consider that an agent, starting in state $S_{t}$ explores the maze by taking an action $A_{t}$, receives the reward $R_{t+1}$ and is transitioned to state $S_{t+1}$. The agent then continues exploring, taking action $A_{t+1}$, and, in similar fashion, receives reward $R_{t+2}$ and is transitioned to state $S_{t+2}$, thus creating a sequence:\n",
    "\n",
    "\\begin{equation*}\n",
    "S_{t},A_{t},R_{t+1},S_{t+1},A_{t+1},R_{t+2},S_{t+2},A_{t+2},R_{t+3}...\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Discounted return, or _utility_ from time step $t$ onwards is the sum of discounted rewards received:\n",
    "\n",
    "\\begin{equation*}\n",
    "U_{t}=R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\gamma^{3} R_{t+4} + ... \n",
    "\\end{equation*}\n",
    "\n",
    "If the agent was following a policy, then in each state $s$, the agent would select the action $\\pi \\left( s\\right)$ defined by that policy and a sequence of states, actions and rewards would arise in similar fashion. \n",
    "\n",
    "The _state-value function_ of a state $s$ under policy $\\pi$, denoted $V^{\\pi }$, is the expected discounted return when following the policy from state $s$ onwards:\n",
    "\n",
    "\\begin{equation*}\n",
    "V^{\\pi }\\left ( s \\right ) =\n",
    "\\mathbb{E}_{\\pi}\\!\\left [\n",
    "    U_{t}\\vert S_{t}=s\n",
    "\\right ]\n",
    "\\end{equation*}\n",
    "\n",
    "Substituting $U_{t}$ and manipulating, we get:\n",
    "\n",
    "\\begin{align*} \n",
    "V^{\\pi }\\!\\left ( s \\right ) \n",
    "&=\n",
    "\\mathbb{E}_{\\pi}\\!\\left [\n",
    " R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\gamma^{3} R_{t+4}... \n",
    "\\vert S_{t}=s\n",
    "\\right ]\n",
    "\\\\\n",
    "&=\n",
    "\\mathbb{E}_{\\pi}\\!\n",
    "\\left [\n",
    "    R_{t+1}+\\gamma \\left( R_{t+2}+\\gamma R_{t+3}+\\gamma^{2} R_{t+4}...    \\right)\n",
    "    \\vert S_{t}=s\n",
    "\\right ]\n",
    "\\\\\n",
    "&=\n",
    "\\mathbb{E}_{\\pi}\\!\n",
    "\\left [\n",
    "    R_{t+1}+\\gamma U_{t+1}\n",
    "    \\vert S_{t}=s\n",
    "\\right ]\n",
    "\\\\\n",
    "&=\n",
    "\\mathbb{E}_{\\pi}\\!\n",
    "\\left [\n",
    "    R_{t+1}+\\gamma V^{\\pi}\\left ( S_{t+1} \\right )\n",
    "    \\vert S_{t}=s\n",
    "\\right ]\n",
    "\\end{align*}\n",
    "\n",
    "Where the last step applies the [Law of iterated expectations](https://en.wikipedia.org/wiki/Law_of_total_expectation) as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}_{\\pi}[V^{\\pi}(S_{t+1}) \\,|\\,S_t=s ] = \\mathbb{E}_{\\pi}[\\mathbb{E}_\\pi[U_{t+1} | S_{t+1}={s}'] \\,|\\,S_t=s ] = \\mathbb{E}_\\pi[U_{t+1} | S_{t}=s]\n",
    "\\end{equation*}\n",
    "\n",
    "State value of $s$ thus decomposes to, first, expected immediate reward received when selecting the action $\\pi \\left( s\\right)$ as defined by the policy and, second, to expected discounted value of the successor state. This reqursive relationship that defines the relation of the value of a state and its successor states is the _Bellman expectation equation_ for $V^{\\pi }$.\n",
    "\n",
    "Applying the MDP constructs laid out above and expanding the expectation gives:\n",
    "\n",
    "\\begin{equation*}\n",
    "V^{\\pi }\\!\\left ( s \\right ) =\n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,\\pi \\left(s \\right ) \\right )\n",
    "\\left( r\\left (s,\\pi \\left(s \\right ),{s}'\\right )  + \\gamma \n",
    "V^{\\pi } \\left ( {s}' \\right )  \\right ) \n",
    "\\end{equation*}\n",
    "\n",
    "Similarly to $V^{\\pi }$, we can define $Q^{\\pi }$, the _action-value function_ for policy $\\pi$ \n",
    "\n",
    "\n",
    "\\begin{align*} \n",
    "Q^{\\pi }\\!\\left ( s, a \\right ) \n",
    "&=\n",
    "\\mathbb{E}_{\\pi} \\left [\n",
    "    U_{t}\\vert S_{t}=s, A_{t}=a\n",
    "\\right ]\n",
    "\\\\\n",
    "\\\\\n",
    "&= \n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( r\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V^{\\pi} \\left ( {s}' \\right )  \\right )\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "For a state-action pair, action-value function $Q^{\\pi }\\!\\left ( s, a \\right )$ defines the expected utility when starting in state $s$, performing action $a$ and following policy $\\pi$ thereafter. After receiving the immediate reward $r\\left (s,a,{s}'\\right )$ the discounted future reward as defined by the successor state value $\\gamma V^{\\pi} \\left ( {s}' \\right )$ is received. This illustrates the recursive relation between $Q^{\\pi }$ and $V^{\\pi }$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal value function\n",
    "\n",
    "Optimal value function is one that gives the best expected utility for each state or state-action pair, $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$, over all policies, defined for state- and action-value functions as:\n",
    "\n",
    "\\begin{equation*}\n",
    "V^{\\ast}\\!\\left ( s \\right ) = \\underset{\\pi}{\\mathrm{max}}\\; V^{\\pi}\\!\\left(s\\right )\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{\\ast}\\!\\left ( s, a \\right ) = \\underset{\\pi}{\\mathrm{max}}\\; Q^{\\pi}\\!\\left(s , a\\right )\n",
    "\\end{equation*}\n",
    "\n",
    "So $V^{\\ast}\\!\\left ( s \\right )$ gives the expected return for being in state $s$ and following optimal policy, while $Q^{\\ast}\\!\\left ( s, a \\right )$ gives the expected return for taking action $a$ in state $s$ and following optimal policy thereafter. \n",
    "\n",
    "_Bellman optimality equation_ for $Q^{\\ast}$ defines a decomposition to immediate return and optimal action-value of the successor state:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{\\ast}\\!\\left ( s, a \\right ) \n",
    "=\n",
    "\\mathbb{E}_{\\pi}\n",
    "\\left [\n",
    "R_{t+1}\n",
    "+\n",
    "\\gamma\\;\n",
    "\\underset{{a}'}{\\mathrm{max}}\\;\n",
    "Q^{\\ast}\\!\n",
    "\\left (\n",
    "S_{t+1},{a}'\n",
    "\\right )\n",
    "\\vert S_{t}=s, A_{t}=a\n",
    "\\right ]\n",
    "\\end{equation*}\n",
    "\n",
    "As the optimal value for state $V^{\\ast}$ must equal the expected utility taking the best action in state $s$ and following optimal policy from that point onward, we can write:\n",
    "\n",
    "\\begin{equation*}\n",
    "V^{\\ast}\\! \\left ( s \\right ) = \\underset{a}{\\mathrm{max}}\n",
    "\\:\n",
    "Q^{\\ast}\\! \\left ( s,a \\right )\n",
    "\\end{equation*}\n",
    "\n",
    "So, for optimal action-value function we have a similar relation as previously discussed for action-value function $Q^{\\pi }$ where we refer to the optimal _state-value function_ of the successor state:\n",
    "\n",
    "\\begin{align*} \n",
    "Q^{\\ast}\\!\\left ( s, a \\right )\n",
    "&=\n",
    "\\mathbb{E}_{\\pi}\n",
    "\\left [\n",
    "    R_{t+1}+\\gamma V^{\\ast}\\left ( S_{t+1} \\right )\n",
    "    \\vert S_{t}=s, A_{t}=a\n",
    "\\right ]\n",
    "\\\\\n",
    "\\\\\n",
    "&= \n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( R\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V^{\\ast} \\left ( {s}' \\right )  \\right )\n",
    "\\end{align*} \n",
    "\n",
    "Combining the above, the optimal state-value function can be expressed as:\n",
    "\n",
    "\\begin{equation*}\n",
    "V^{\\ast}\\!\\left ( s \\right ) = \\underset{a}{\\mathrm{max}}\n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( r\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V^{\\ast} \\left ( {s}' \\right )  \\right )\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal policy\n",
    "\n",
    "We consider a policy $\\pi_{1}$ to be equal or better than some other policy $\\pi_{2}$, if the state-value in all states is at least as good for $\\pi_{1}$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi_{1} \\geq \\pi_{2} \\; \\Leftrightarrow \\; V^{\\pi_{1}}(s) \\geq V^{\\pi_{2}}(s), \\;\\forall s \\in \\mathcal{S}\n",
    "\\end{equation*}\n",
    "\n",
    "For any MDP, there always exists an _optimal deterministic policy_ that is at least as good as all other policies:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi^{\\ast} \\geq \\pi, \\; \\forall \\pi\n",
    "\\end{equation*}\n",
    "\n",
    "There can be multiple optimal policies, but all optimal policies achieve the optimal value functions $V^{\\ast}(s)$ and $Q^{\\ast}(s,a)$.\n",
    "\n",
    "So, finally, optimal policy is the policy (or one of the best policies) that gives the best value for each state:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi^{\\ast}\\!\\left ( s \\right ) = \\underset{a}{\\mathrm{argmax}}\\; \n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( R\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V^{\\ast} \\left ( {s}' \\right )  \\right )\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation and update rules\n",
    "\n",
    "The task of computing state-value function for states under policy $\\pi$ is called _policy evaluation_. Considering the formula for $V^{\\pi }$ above, we see that we we have an equation for each state $V^{\\pi}\\!\\left ( s \\right )$ that refers to values of subsequent states. This gives as a system of $\\left |\\mathcal{S}\\right |$ simultaneous linear equations with $\\left |\\mathcal{S}\\right |$ unknown state values $V^{\\pi}\\!\\left ( s \\right )$ which can be solved with a suitable standard method. \n",
    "\n",
    "### Bellman expectation backup\n",
    "\n",
    "An iterative solution method for policy evaluation can be derived by using the Bellman expectation equation for state-values as an update rule, also called _Bellman expectation backup_: \n",
    "\n",
    "\\begin{equation*}\n",
    "V^{\\pi}_{k+1 }\\!\\left ( s \\right ) =\n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,\\pi \\left(s \\right ) \\right )\n",
    "\\left( r\\left (s,\\pi \\left(s \\right ),{s}'\\right )  + \\gamma \n",
    "V^{\\pi}_{k}\\left ( {s}' \\right )  \\right ) \n",
    "\\end{equation*}\n",
    "\n",
    "We update state-value $V^{\\pi}_{k+1 }\\!\\left ( s \\right )$ based on old values of the successor states $V^{\\pi}_{k}\\left ( {s}' \\right )$ and the expected immediate rewards. We would sweep thought the state set, updating each state in turn (a synchronous backup). This is called _iterative policy evaluation_ and is guaranteed to converge to $V^{\\pi}$.\n",
    "\n",
    "Later, we consider a matrix formulation for policy evaluation (rather than the iterative solution discussed above) in the implementation of policy iteration algorithm, see [here](maze_policy_iteration.ipynb#Matrix-implementation-of-policy-evaluation).\n",
    "\n",
    "### Bellman optimality backup\n",
    "\n",
    "In similar fashion, to find an optimal policy we can derive an update rule from Bellman optimality equation as:\n",
    "\n",
    "\\begin{equation*}\n",
    "V_{k+1 }\\!\\left ( s \\right ) =\n",
    "\\underset{a}{\\mathrm{max}}\n",
    "\\:\n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( r\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V_{k}\\left ( {s}' \\right )  \\right ) \n",
    "\\end{equation*}\n",
    "\n",
    "This _Bellman optimality backup_ is the building block for value iteration algorithm exhibited in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration algorithm\n",
    "\n",
    "The goal for Value iteration algorithm is to find the optimal state-value function $V^{\\ast}$ for a fully defined MDP and derive the corresponding optimal policy $\\pi^{\\ast}$ from the value function.\n",
    "\n",
    "The algorithm performs synchronous sweeps of the state-space and updates each state in turn by applying the Bellman optimality backup. \n",
    "\n",
    "For arbitary $V_{0}$, value iteration algorithm can be shown to convergence to $V^{\\ast}$ for discounted finite MDPs if $\\gamma < 1$ or if the state-action-reward sequences (episodes) are guaranteed to be finite. In practice, we consider the algorithm has converged when the maximum change between two subsequent value functions is smaller than some predefined threshold $\\theta$.\n",
    "\n",
    "Once the algorithm has converged to a (near) optimal solution, we determine the optimal policy $\\pi^{\\ast}$ corresponding to the value function by looping through all states and determining the action that maximizes the expected returns from that state.\n",
    "\n",
    "Pseudocode for the algorithm could stand as follows:\n",
    "\n",
    "<span style=\"font-family: monospace;\">\n",
    "    \n",
    "> **Value iteration:** \n",
    ">   \n",
    "> initialize $\\theta > 0$, $k =0$ and $V_{0}\\left ( s \\right ) \\in \\mathbb{R}$ for all $S$\n",
    "> \n",
    "> loop: \n",
    ">> for each $s \\in \\mathbb{S}$:\n",
    ">>> update $\n",
    "V_{k+1 }\\!\\left ( s \\right ) \\leftarrow\n",
    "\\underset{a}{\\mathrm{max}}\n",
    "\\:\n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( r\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V_{k}\\left ( {s}' \\right )  \\right ) \n",
    "$\n",
    ">>\n",
    ">> $k = k + 1$\n",
    ">\n",
    "> until convergence, $\\left \\| V_{k+1} - V_{k} \\right \\|_{\\infty}<\\theta$\n",
    "> \n",
    "> return $V^{\\ast} \\approx V_{k+1}$\n",
    ">\n",
    ">\n",
    "> **Policy extraction:** \n",
    ">    \n",
    "> for each $s \\in \\mathbb{S}$:\n",
    ">> extract policy, $\n",
    "\\pi^{\\ast}\\!\\left ( s \\right ) \\leftarrow \\underset{a}{\\mathrm{argmax}}\\; \n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( R\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V^{\\ast} \\left ( {s}' \\right )  \\right )\n",
    "$\n",
    "> \n",
    "> return $\\pi^{\\ast}$\n",
    "\n",
    "</span>\n",
    "\n",
    "For this implementation, we need to maintain two copies of state values; current values $V_{k}$ and updated values $V_{k+1}$. Indeed, we use the updated values only during the next iteration, when all states have been visited and updated.\n",
    "\n",
    "It is good to note that there is no need to represent the policy during execution of the algorithm. The updates refer to state values only, and policy is only extracted at the end.\n",
    " \n",
    "Computational complexity is $O(\\left |\\mathcal{A}\\right |\\left |\\mathcal{S}\\right |^{2})$ per iteration for $\\left |\\mathcal{A}\\right |$ actions and $\\left |\\mathcal{S}\\right |$ states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration implementation\n",
    "\n",
    "In the following, we illustrate an implementation of Value Iteration algorithm. As the goal is to illustrate the algorithm using the maze as an example, no effort has been put into generalizing the implementation. Rather, we utilize `Maze` specific concepts such as `Movement` directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_best_action` enables calculating the state-value corresponding to best action from that state for the update step:\n",
    "\n",
    "\\begin{equation*}\n",
    "V_{k+1 }\\!\\left ( s \\right ) \\leftarrow\n",
    "\\underset{a}{\\mathrm{max}}\n",
    "\\:\n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( r\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V_{k}\\left ( {s}' \\right )  \\right ) \n",
    "\\end{equation*}\n",
    "\n",
    "and is also used for extracting the best actions for optimal policy:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi^{\\ast}\\!\\left ( s \\right ) \\leftarrow \\underset{a}{\\mathrm{argmax}}\\; \n",
    "\\sum_{{s}'\\in\\mathcal{S}}\n",
    "P\\left( {s}' \\vert s,a \\right )\n",
    "\\left( r\\left (s,a,{s}'\\right )  + \\gamma \n",
    "V^{\\ast} \\left ( {s}' \\right )  \\right )\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the implementation is lazy; For several equally good actions, the first one is returned.\n",
    "\n",
    "Note also that for our simple maze, the reward only depends on `s_prime`, the state entered, so `maze.rewards[s_prime]` is sufficient for finding the immediate reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(maze, movement, state, state_values):\n",
    "   \n",
    "    best_action = None\n",
    "    best_value = -math.inf\n",
    "\n",
    "    for action in movement.actions:\n",
    "\n",
    "        val = 0\n",
    "\n",
    "        for move_direction, p_move in movement.get_direction_probs(action):\n",
    "\n",
    "            s_prime = movement.move_from(state, move_direction)\n",
    "\n",
    "            reward = maze.living_cost + maze.rewards[s_prime]\n",
    "            s_prime_value = state_values[s_prime]\n",
    "\n",
    "            val += p_move * (reward + maze.gamma * s_prime_value)\n",
    "\n",
    "        if val > best_value:\n",
    "            best_value = val\n",
    "            best_action = action\n",
    "\n",
    "    return best_value, best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extract_policy` sweeps through states and finds the optimal action for each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(maze, movement, state_values):\n",
    "    \n",
    "    policy = {}\n",
    "    \n",
    "    for state in maze.get_iterator(\"states\"):\n",
    "        \n",
    "        if maze.terminal[state]:\n",
    "            continue\n",
    "        \n",
    "        best_value, best_action = get_best_action(maze, movement, state, state_values)\n",
    "\n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value_iteration_sweep`performs one iteration of the algorithm, sweeping through all states and updating the values in turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_sweep(maze, movement, state_values):\n",
    "    \n",
    "    values = state_values.copy()\n",
    "    \n",
    "    for state in maze.get_iterator(\"states\"):\n",
    "\n",
    "        if maze.terminal[state]:\n",
    "            continue\n",
    "        \n",
    "        best_value, _ = get_best_action(maze, movement, state, state_values)\n",
    "        values[state] = best_value\n",
    "        \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `distance` between two value functions, $V_{k+1}$ and $V_{k}$, we use the $\\infty$-norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(list1, list2):\n",
    "    \n",
    "    # for np.linalg.norm, ord=np.inf for vector x is max(abs(x))\n",
    "    \n",
    "    return np.linalg.norm(np.subtract(list1, list2), ord=np.inf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the entry point to main loop allows us to terminate either based on $\\theta$ (`C_LIMIT`), or based on max number of iterations (`MAX_ITERS`), whichever is reached first. \n",
    "\n",
    "We can choose to plot values for each iteration by giving a suitable plotter function for `plotter` keyword argument, such as `plot_maze` utility function defined in Maze basis implementation notebook. In that case, we also extract the policy corresponding to the current value-function. As noted eaerlier, intermediate policies are not otherwise needed by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(maze, movement, initial_values, *, MAX_ITERS=10, C_LIMIT = 0.01, plotter=None):\n",
    "\n",
    "    values = initial_values.copy()\n",
    "\n",
    "    for i in range(0, MAX_ITERS):\n",
    "        \n",
    "        updated_values = value_iteration_sweep(maze, movement, values)\n",
    "        \n",
    "        if plotter:\n",
    "            policy = extract_policy(maze, movement, updated_values)\n",
    "            plotter(maze, updated_values, policy)\n",
    "\n",
    "        previous_values_list = maze.get_as_list(values)\n",
    "        updated_values_list = maze.get_as_list(updated_values)\n",
    "        \n",
    "        dist = distance(previous_values_list, updated_values_list)\n",
    "        \n",
    "        print(f\"ROUND {i}, {dist}\")\n",
    "\n",
    "        if dist <= C_LIMIT:\n",
    "            break\n",
    "\n",
    "        values = updated_values\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: The Canonical maze\n",
    "\n",
    "We illustrate the algorithm using the canonical maze as an example. We repeat the definition already given earlier.\n",
    "\n",
    "First, we intialize the maze with two final states. We start with `gamma=1`, `living_cost = -0.04` and `noise=0.2`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_config = {\n",
    "    'size': (3, 4),\n",
    "    'walls': [(1,1)],\n",
    "    'terminal_states': [(0,3), (1,3)],\n",
    "    'rewards': {\n",
    "        (0,3): 1,\n",
    "        (1,3): -1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maze = Maze(maze_config, gamma=1, living_cost = -0.04)\n",
    "movement = Movement(maze, noise=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maze would look as follows, with rewards diplayed for each state (taking in account the reward associated with entering the state as well as the living cost):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_rewards(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We intialize the state-values to zero, and for illustration, extract the policy corresponding to initial values. Note that  displaying NORTH, the first action for all states not adjacent to a terminal state (with a non-zero reward), is a consequence of not actively resolving ties in the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_values = { state:0 for state in maze.get_iterator(\"states\") }\n",
    "display(initial_values)\n",
    "\n",
    "initial_policy = extract_policy(maze, movement, initial_values)\n",
    "display({r:Movement.action_names[initial_policy[r]] for r in maze.get_iterator(\"states\") if r in initial_policy})\n",
    "\n",
    "plot_maze(maze, initial_values, initial_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run the algorithm. We set target as $\\theta=0.0001$ and hope to converge in 50 iterations. `plot_maze` will be employed to display state-values and corresponding policy actions for each iteration step below.\n",
    "\n",
    "We would expect the policy to converge to optimal policy faster than the state-values converge. This can be verified by comparing the images rendered during iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "values = value_iteration(maze, movement, initial_values,\n",
    "                         MAX_ITERS=50, C_LIMIT = 0.0001, \n",
    "                         plotter=plot_maze)\n",
    "policy = extract_policy(maze, movement, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the algorithm has converged, we can display the optimal state-values and corresponding policy for the canonical maze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(values)\n",
    "display({r:Movement.action_names[policy[r]] for r in maze.get_iterator(\"states\") if r in policy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: a slightly more complicated maze\n",
    "\n",
    "As a second example, we run through corresponding steps for a sligthly more complicated maze, again using `gamma=1`, `living_cost=-0.04` and `noise=0.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_config2 = {\n",
    "    'size': (5, 7),\n",
    "    'walls': [(1,1), (1,2), (1,3), (2,1), (3,1),\n",
    "              (3,3), (3,4), (3,5), (2,5), (3,5) ],\n",
    "    'terminal_states': [(2,3), (1,5)],\n",
    "    'rewards': {\n",
    "        (2,3): 1,\n",
    "        (1,5): -1\n",
    "    }\n",
    "}\n",
    "maze2 = Maze(maze_config2, gamma=1, living_cost = -0.04)\n",
    "movement2 = Movement(maze2, noise=0.2)\n",
    "\n",
    "plot_state_rewards(maze2, ax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize state-values to zero and run `value_iteration` for max 50 iterations or until reaching treshold of $\\theta$= 0.0001.\n",
    "\n",
    "Note that this time omit the `plotter` argument and thus skip plotting intermediate value functions or policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initial_values2 = { state:0 for state in maze2.get_iterator(\"states\") }\n",
    "\n",
    "values2 = value_iteration(maze2, movement2, initial_values2, \n",
    "                          MAX_ITERS=50, C_LIMIT = 0.0001)\n",
    "policy2 = extract_policy(maze2, movement2, values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the (near) optimal state-values, we can plot the values as well as the corresponding optimal policy on the maze grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_values(maze2, values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_policy_actions(maze2, policy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final example: a complex maze\n",
    "\n",
    "As final example, we create an even more complex maze of almost mindboggling complexity, with multiple terminal states and this time apply a discount factor as well. We use the following definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_config3 = {\n",
    "    'size': (8, 7),\n",
    "    'walls': [ (1,1), (1,2), (1,4), \n",
    "              (2,1), (2,4), \n",
    "              (4,3),(4,5), (4,6),\n",
    "              (5,2),\n",
    "              (6,3), (6,4), (6,5),\n",
    "              (7,1) ],\n",
    "    'terminal_states': [ (1,5), (2,2), (2,5), (4,1), (4,2), (5,1), (5,3) ],\n",
    "    'rewards': {\n",
    "        (1,5): -1,\n",
    "        (2,2): -1,\n",
    "        (2,5): -1,\n",
    "        (4,1): -1,\n",
    "        (4,2): -1,\n",
    "        (5,3):  1,\n",
    "        (5,1): -1,\n",
    "    }\n",
    "}\n",
    "maze3 = Maze(maze_config3, gamma=0.9, living_cost = -0.01)\n",
    "movement3 = Movement(maze3, noise=0.2)\n",
    "\n",
    "initial_values3 = { state:0 for state in maze3.get_iterator(\"states\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_rewards(maze3, ax=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "values3 = value_iteration(maze3, movement3, initial_values3,\n",
    "                          MAX_ITERS=1000, C_LIMIT = 0.0001)\n",
    "policy3 = extract_policy(maze3, movement3, values3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_values(maze3, values3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_actions(maze3, policy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing it\n",
    "\n",
    "Finally, we time the execution of the algorithm to get an idea of the relative computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture c\n",
    "%%timeit -o \n",
    "\n",
    "values3 = value_iteration(maze3, movement3, initial_values3,\n",
    "                          MAX_ITERS=1000, C_LIMIT = 0.0001)\n",
    "policy3 = extract_policy(maze3, movement3, values3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = r'.*$'\n",
    "timeitresults = re.findall(regex, c.stdout)\n",
    "print(timeitresults[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the result to policy iteration in the [end of the notebook](./maze_policy_iteration.ipynb#Timing-the-execution) \n",
    "covering policy iteration implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
