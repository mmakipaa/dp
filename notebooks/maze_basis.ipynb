{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple maze\n",
    "\n",
    "This notebook covers the implementation of a simple _maze_ (or _grid-world_) to facilitate experimenting with basic dynamic programming algorithms, such as value iteration and policy iteration.\n",
    "\n",
    "The building blocks defined in this notebook can be reused in another notebook using module importer from __[ipynb](https://github.com/ipython/ipynb)__ package, see __[here](https://ipynb.readthedocs.io/en/latest/)__. Import is best done _definitions only_ so that this notebook is not exceuted and output of the notebook does not get cluttered with examples provided here. For instance:\n",
    "\n",
    "```python\n",
    "from ipynb.fs.defs._filename_of_this_notebook_ import (\n",
    "    Maze, Movement,\n",
    "    plot_maze, plot_policy_actions, plot_state_rewards, plot_state_values\n",
    ")\n",
    "```  \n",
    "\n",
    "With this, import statements, class and function definitions as well as constants defined in ALL_CAPS will get imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The maze \n",
    "\n",
    "### Maze class\n",
    "\n",
    "We define a simple `Maze` class to contain the structure of the grid world. In addition to the structure of the maze, such as size, walls and terminal states, we consider the rewards received when exploring the maze to be a part of the maze definition. Hence the class defines the reward received when entering a state, and takes into account discounting and living cost.\n",
    "\n",
    "State values (determined by some algorithm, such as value iteration) or resulting policy are not considered to be a part of the maze, and are managed in separate data structures outside the `Maze` class.\n",
    "\n",
    "The structure of the maze is internally represented as Numpy [ndarrays](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) and states can be indexed with `(x,y)` tuples, starting from top left corner as `(0,0)`.\n",
    "\n",
    "`Maze` constructor takes a `maze_config` dictionary that defines the maze structure as an argument and stores the needed internal structures. \n",
    "\n",
    "In addition, we give the value of discounting parameter $\\gamma$ and a living cost parameter as arguments to Maze class constructor.\n",
    "\n",
    "See below for example use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    def __init__(self, maze_config, *, gamma=1, living_cost=0):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.living_cost = living_cost\n",
    "        \n",
    "        self.size = maze_config['size']\n",
    "\n",
    "        self.walls = np.full(self.size, False, dtype=bool)\n",
    "        self.walls[tuple(zip(*maze_config['walls']))] = True\n",
    "        \n",
    "        # Note: default reward for entering a state is 0\n",
    "        \n",
    "        self.rewards = np.full(self.size, 0, dtype=np.float32)\n",
    "        self.rewards[self.walls] = np.nan\n",
    "        \n",
    "        for key in maze_config['rewards']:\n",
    "            self.rewards[key] = maze_config['rewards'][key]\n",
    "        \n",
    "        self.terminal = np.full(self.size, False, dtype=bool)\n",
    "        self.terminal[tuple(zip(*maze_config['terminal_states']))] = True\n",
    "        \n",
    "        self.states = {}\n",
    "        self.grid_index = []\n",
    "        for index, wall in np.ndenumerate(self.walls):\n",
    "            if not wall:\n",
    "                self.grid_index.append(index)\n",
    "                self.states[index] = index\n",
    "        \n",
    "        self.state_count = len(self.grid_index)\n",
    "\n",
    "    def get_iterator(self, param):\n",
    "        return MazeIterator(self, param)\n",
    "    \n",
    "    def get_as_list(self, param):\n",
    "        return [ r for r in self.get_iterator(param) ]\n",
    "    \n",
    "    def is_valid_state(self, state):\n",
    "        # short circuit order\n",
    "        return (0 <= state[0] <= self.size[0] - 1 and 0 <= state[1] <= self.size[1] - 1) and (not self.walls[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze iterator\n",
    "\n",
    "In addition, we define an __[iterator](https://wiki.python.org/moin/Iterator)__ to enumerate the maze grid states that the agent can enter (i.e. the ones that do not contain walls). Even though we will be doing a lot of iterating, this is not only for convenience,  but also to help avoid bugs potentially difficult to trace: To guarantee, that we always consider the states in same order, even when processing data structures external to the maze.\n",
    "\n",
    "The iterator can be used to iterate maze properties with string references to `Maze` class properties, e.g.:\n",
    "```\n",
    "for state in maze.get_iterator(\"states\"):\n",
    "    # each state \n",
    "    \n",
    "print(list(maze.get_iterator('rewards')))\n",
    "\n",
    "```    \n",
    "Or to iterate external data structures in the order defined by `Maze.grid_index`:\n",
    "```\n",
    "initial_values = { state:0 for state in maze.get_iterator(\"states\") }\n",
    "list(maze.get_iterator(initial_values))\n",
    "```\n",
    "Or even\n",
    "```\n",
    "maze.get_as_list(initial_values)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeIterator:\n",
    "    def __init__(self, maze, param):\n",
    "        self.grid_index = maze.grid_index\n",
    "        self.max = len(self.grid_index)\n",
    "        \n",
    "        if type(param) is str:\n",
    "            self.param = getattr(maze, param)\n",
    "        else:\n",
    "            self.param = param\n",
    "        \n",
    "        self.n = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n >= self.max:\n",
    "            raise StopIteration\n",
    " \n",
    "        res = self.param[self.grid_index[self.n]]\n",
    "        \n",
    "        self.n += 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movement within the maze\n",
    "\n",
    "We collect the logic of agent's movement within the grid world into `Movement` utility class. In addition, together with `Maze`, `Movement`class enables determining the transition probabilities for when we analyze the maze as a Markov Decision Process (MDP).\n",
    "\n",
    "The agent may attempt to move to one of four directions: `NORTH`, `EAST`, `SOUTH` and `WEST`. \n",
    "\n",
    "Movement is noisy. Attempt to move to certain direction (e.g. _north_) may result in ending up left of the intended direction (e.g. _west_ if intended direction was _north_), or right of the direction (e.g. _east_).\n",
    "\n",
    "For action _north_, `movement.get_direction_probs(action)` would return directions _west_, _north_ and _east_, with probabilities 0.1, 0.8 and 0.1, respectively, assuming $noise = 0.2$. \n",
    "\n",
    "If there is a wall blocking the move or the move ends up outside the maze grid, `move_from` returns to current state. Otherwise, the new state in the move direction is returned. To see where a move from current state to given direction would end up, agent would perform\n",
    "\n",
    "```\n",
    "s_prime = movement.move_from(current_state, move_direction)\n",
    "```\n",
    "\n",
    "Utilizing the class, an agent can analyze the movement directions and find probabilities associated with follow up states when trying to perform a particular `action`. For instance, to analyze an action available in a state, an agent could perform:\n",
    "\n",
    "```\n",
    "for move_direction, p_move in movement.get_direction_probs(action):\n",
    "    \n",
    "    s_prime = movement.move_from(state, move_direction)\n",
    "    reward = maze.living_cost + maze.rewards[s_prime]\n",
    "    s_prime_value = state_values[s_prime]\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Movement:\n",
    "\n",
    "    NORTH = 0\n",
    "    EAST = 1\n",
    "    SOUTH = 2\n",
    "    WEST = 3\n",
    "\n",
    "    actions = [ NORTH, EAST, SOUTH, WEST ]\n",
    "    action_names = [ \"NORTH\", \"EAST\", \"SOUTH\", \"WEST\" ]\n",
    "    direction_arrows = [ '\\u2191','\\u2192','\\u2193','\\u2190', '\\u25aa', ''] # up, right, down, left, small square, empty\n",
    "\n",
    "    def __init__(self, maze, *, noise=0.2):\n",
    "        \n",
    "        self.maze = maze\n",
    "        self.NOISE = noise\n",
    "        \n",
    "        self.noisy_moves = [\n",
    "            self._adjust_left,\n",
    "            self._adjust_none,\n",
    "            self._adjust_right\n",
    "        ]\n",
    "        self.noisy_move_probs = [self.NOISE / 2, 1 - self.NOISE, self.NOISE / 2] # left, straight, right\n",
    "        \n",
    "\n",
    "    def _adjust_left(cls, direction):\n",
    "        return (cls.actions.index(direction) + len(cls.actions) - 1) % len(cls.actions)\n",
    "\n",
    "    def _adjust_none(cls, direction):\n",
    "        return direction\n",
    "    \n",
    "    def _adjust_right(cls, direction):\n",
    "        return (cls.actions.index(direction) + 1) % len(cls.actions)\n",
    "\n",
    "    def get_direction_probs(self, action):\n",
    "        dirs = []\n",
    "        for j, _ in enumerate(self.noisy_moves):\n",
    "            p_move = self.noisy_move_probs[j]\n",
    "            move_direction = self.noisy_moves[j](action)\n",
    "            \n",
    "            dirs.append((move_direction, p_move))\n",
    "        \n",
    "        return dirs\n",
    "    \n",
    "    def _get_move_target(self, from_state, action):\n",
    "\n",
    "        if action == Movement.NORTH:\n",
    "            target_state = (from_state[0] - 1, from_state[1])\n",
    "        elif action == Movement.EAST:\n",
    "            target_state = (from_state[0], from_state[1] + 1)\n",
    "        elif action == Movement.SOUTH:\n",
    "            target_state = (from_state[0] + 1, from_state[1])\n",
    "        elif action == Movement.WEST:\n",
    "            target_state = (from_state[0], from_state[1] - 1)\n",
    "\n",
    "        return target_state\n",
    "\n",
    "    def move_from(self, from_state, action):\n",
    "\n",
    "        target_state = self._get_move_target(from_state, action)\n",
    "\n",
    "        # check if trying to move outside the maze or against a wall\n",
    "        # and in that case stay in current position\n",
    "        \n",
    "        if self.maze.is_valid_state(target_state):\n",
    "            return target_state\n",
    "        else:\n",
    "            return from_state        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the maze\n",
    "\n",
    "We define helper functions for visualizing the maze. First, `plot_maze_grid` is the utility function to create a __[Seaborn heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html?highlight=heatmap#seaborn.heatmap)__ based visualization of the maze and is used by the other helper functions providing different annotations and colormaps for customization.\n",
    "\n",
    "To annotate the grid with rewards, we use `plot_state_rewards`. To show current state values on the grid, we pass the state values to `plot_state_values`. Similarly, `plot_policy_actions` allows for plotting direction arrows corresponding to a policy given as argument on the maze grid.\n",
    "\n",
    "Finally, `plot_maze` will allow for plotting current state values and policy actions side by side in two subplots.\n",
    "\n",
    "We define two colormaps as `LinearSegmentedColormap` objects that are instantiated by giving a list of preselected colors. First colormap, `CM_VALUES` is used for representing the walls and terminal states of the maze. Here it is assumed that entering good terminal states provides a positive (colored light green) and entering bad terminal states a negative reward (colored light red) . The other, `CM_ACTIONS` is used as a background to gently differientate policy actions. The colormaps are illustrated below.\n",
    "\n",
    "Some rudimentary scaling of annotation texts is done to facilitate plotting of diffent size grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a figure size constant that would create a 1280 x 720 image with matplotlib default density of 100 dpi\n",
    "\n",
    "FIG_SIZE=(12.8, 7.2)\n",
    "FIG_DPI=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_COLORS = ['#f7f7f7','#a1d76a', '#e9a3c9', '#999999'] # NONE, GOOD, BAD, WALL\n",
    "CM_VALUES = LinearSegmentedColormap.from_list(\"value\", VALUE_COLORS, N=4)\n",
    "\n",
    "ACTION_COLORS = ['#fcfefe', '#edf7f8', '#d5e4f2', '#dfedf5','#91bfdb','#999999'] # N, E, S, W, TERMINAL, WALL \n",
    "CM_ACTIONS = LinearSegmentedColormap.from_list(\"action\", ACTION_COLORS, N=6)\n",
    "\n",
    "fig = plt.figure(figsize=tuple(i/1.6 for i in FIG_SIZE), dpi=FIG_DPI)\n",
    "\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "sns.heatmap([list(range(0,4))], fmt='', cmap=CM_VALUES, \\\n",
    "                 linewidths=0, rasterized=False, \\\n",
    "                 cbar=False, ax=ax)\n",
    "\n",
    "ax = fig.add_subplot(2, 1, 2)\n",
    "sns.heatmap([list(range(0,6))], fmt='', cmap=CM_ACTIONS, \\\n",
    "                 linewidths=0, rasterized=False, \\\n",
    "                 cbar=False, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_text(maze_size):\n",
    "    return 18 / max(1, max(maze_size) / 6)\n",
    "\n",
    "def scale_arrows(maze_size):\n",
    "    return 32 / max(1, max(maze_size) / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_maze_grid(values, annotations, *, annot_kws={'size': 16}, cm, ax=None):\n",
    "    \n",
    "    if not ax:\n",
    "        plt.figure(figsize=FIG_SIZE, dpi=FIG_DPI)\n",
    "        \n",
    "    ax = sns.heatmap(values, annot=annotations, annot_kws=annot_kws, fmt='', cmap=cm, \\\n",
    "                     square=True, linewidths=0.01, linecolor='#5f5f5f', rasterized=False, \\\n",
    "                     cbar=False, ax=ax)\n",
    "    ax.tick_params(left=False, bottom=False) \n",
    "    ax.tick_params(labelleft=False, labelbottom=False)\n",
    "\n",
    "    sns.despine(ax=ax, top=False, right=False, left=False, bottom=False)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state_rewards(maze, *, ax=None, cm=CM_VALUES):\n",
    "\n",
    "    NONE = 0\n",
    "    GOOD = 1\n",
    "    BAD = 2\n",
    "    WALL = 3\n",
    "\n",
    "    cells = np.full(maze.size, NONE)\n",
    "    cells[maze.rewards > 0] = GOOD\n",
    "    cells[maze.rewards < 0] = BAD\n",
    "    cells[maze.walls] = WALL\n",
    "   \n",
    "    annot_text = np.full(maze.size, '', dtype=object)\n",
    "    \n",
    "    keys = maze.get_as_list(\"states\")\n",
    "\n",
    "    for key in keys:\n",
    "        r = maze.rewards[key] \n",
    "        annot_text[key] = f\"{r + maze.living_cost:.3f}\"\n",
    "\n",
    "    annot_kws={'size': scale_text(maze.size)}\n",
    "    \n",
    "    plot_maze_grid(cells, annot_text, annot_kws=annot_kws, cm=cm, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state_values(maze, state_values, *, ax=None, cm=CM_VALUES):\n",
    "\n",
    "    NONE = 0\n",
    "    GOOD = 1\n",
    "    BAD = 2\n",
    "    WALL = 3\n",
    "    \n",
    "    cells = np.full(maze.size, NONE)\n",
    "    cells[maze.rewards > 0] = GOOD\n",
    "    cells[maze.rewards < 0] = BAD\n",
    "    cells[maze.walls] = WALL\n",
    "    \n",
    "    annot_text = np.full(maze.size, '', dtype=object)\n",
    "\n",
    "    for a, key in enumerate(state_values):\n",
    "        val = state_values[key]\n",
    "        annot_text[key] =  f\"{val:.3f}\"\n",
    "    \n",
    "    annot_kws={'size': scale_text(maze.size)}\n",
    "    \n",
    "    plot_maze_grid(cells, annot_text, annot_kws=annot_kws, cm=cm, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_actions(maze, policy, *, ax=None, cm=CM_ACTIONS):\n",
    "    \n",
    "    # 0...3 = NORTH, EAST, ...\n",
    "    TERMINAL = 4\n",
    "    WALL = 5\n",
    "    \n",
    "    cells = np.full(maze.size, WALL)\n",
    "    for a, key in enumerate(policy):\n",
    "        cells[key] = policy[key]\n",
    "\n",
    "    cells[maze.terminal] = TERMINAL\n",
    "\n",
    "    ax = plot_maze_grid(cells, False, cm=cm, ax=ax)\n",
    "    \n",
    "    arrow_size = scale_arrows(maze.size)\n",
    "    \n",
    "    for index, c in np.ndenumerate(cells):\n",
    "        arrow = Movement.direction_arrows[c]\n",
    "        ax.text(index[1] + 0.5, index[0] + 0.5, arrow, size=arrow_size, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_maze(maze, state_values, policy):\n",
    "    fig = plt.figure(figsize=FIG_SIZE, dpi=FIG_DPI)\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    plot_state_values(maze, state_values, ax=ax)\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plot_policy_actions(maze, policy, ax=ax)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example use: Canonical maze\n",
    "\n",
    "To instantiate a `maze`, we define a `maze_config` dictionary, that defines the size of the maze, location of walls within the grid, the terminal states and rewards associated when moving to the state in question.\n",
    "\n",
    "To illustrate, we create the \"canonical maze\" used commonly in reinforcement learning examples and plot the rewards associated with states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_config = {\n",
    "    'size': (3, 4),\n",
    "    'walls': [(1,1)],\n",
    "    'terminal_states': [(0,3), (1,3)],\n",
    "    'rewards': {\n",
    "        (0,3): 1,\n",
    "        (1,3): -1\n",
    "    }\n",
    "}\n",
    "maze = Maze(maze_config, gamma=1, living_cost = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_state_rewards(maze, ax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second example: a slightly more complicated maze\n",
    "\n",
    "As another example, we create a slighly more complicated maze with more walls, but two terminal states with rewards -1 and 1 as before. We also add a living cost of -0.04 for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maze_config = {\n",
    "    'size': (5, 7),\n",
    "    'walls': [(1,1), (1,2), (1,3), (2,1), (3,1),\n",
    "              (3,3), (3,4), (3,5), (2,5), (3,5) ],\n",
    "    'terminal_states': [(2,3), (1,5)],\n",
    "    'rewards': {\n",
    "        (2,3): 1,\n",
    "        (1,5): -1\n",
    "    }\n",
    "}\n",
    "maze = Maze(maze_config, gamma=0.9, living_cost = -0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_state_rewards(maze, ax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: State values displayed on maze grid\n",
    "\n",
    "We initialize the state values to random values and show the values on the maze grid. Note that we consider value of terminal states to be zero as there will be no future rewards available after entering a terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_values = { state: 0 if maze.terminal[state] else np.random.rand() for state in maze.get_iterator(\"states\") }\n",
    "\n",
    "display(random_values)\n",
    "plot_state_values(maze, random_values, ax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:  Policy actions displayed on maze grid\n",
    "\n",
    "As the final example, we create a random policy and visualize the policy on the maze grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "\n",
    "for state in maze.get_iterator(\"states\"):\n",
    "\n",
    "    if maze.terminal[state]:\n",
    "        continue\n",
    "\n",
    "    policy[state] = np.random.choice(Movement.actions)\n",
    "\n",
    "display(policy)\n",
    "plot_policy_actions(maze, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
